"""Ollama æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹ã€‚"""

import asyncio

from src.agent import Agent, AgentSettings, LLMSettings


async def main():
    """Ollama ä½¿ç”¨ç¤ºä¾‹ã€‚"""
    print("=== Ollama æœ¬åœ°æ¨¡å‹ç¤ºä¾‹ ===")

    # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦å¯ç”¨
    try:
        import requests

        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code != 200:
            print("âŒ é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
            print("è¯·ç¡®ä¿ï¼š")
            print("1. å·²å®‰è£… Ollama: https://ollama.com")
            print("2. å·²å¯åŠ¨æœåŠ¡: ollama serve")
            print("3. å·²ä¸‹è½½æ¨¡å‹: ollama pull llama3.2")
            return

        # è·å–å¯ç”¨æ¨¡å‹
        models = response.json().get("models", [])
        print(f"âœ… å‘ç° {len(models)} ä¸ªæœ¬åœ°æ¨¡å‹:")
        for model in models:
            print(f"   - {model['name']} ({model.get('size', 'Unknown')})")

        if not models:
            print("âŒ æ²¡æœ‰å‘ç°æœ¬åœ°æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½ï¼š")
            print("   ollama pull llama3.2")
            return

    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama: {e}")
        print("è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve")
        return

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ï¼Œæˆ–é»˜è®¤ä¸º llama3.2
    model_name = models[0]["name"] if models else "llama3.2"
    print(f"\nğŸš€ ä½¿ç”¨æ¨¡å‹: {model_name}")

    # é…ç½®æ™ºèƒ½ä½“ä½¿ç”¨ Ollama
    settings = AgentSettings(
        name="ollama_demo",
        instruction="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¿è¡Œåœ¨ç”¨æˆ·çš„æœ¬åœ°è®¡ç®—æœºä¸Šã€‚ä½ çš„å›ç­”è¦ç®€æ´æ˜äº†ã€‚",
        llm_settings=LLMSettings(
            provider="ollama",
            model=model_name,
            base_url="http://localhost:11434",
            temperature=0.7,
            max_tokens=512,
        ),
        # å¯ä»¥æ·»åŠ ä¸€äº›MCPå·¥å…·
        mcp_servers={
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿æœ‰å¯ç”¨çš„MCPæœåŠ¡å™¨
        },
    )

    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = Agent(config=settings)

    try:
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        print("\nğŸ“¡ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½ä½“...")
        await agent.initialize()
        print("âœ… æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")

        # ç¤ºä¾‹1ï¼šç®€å•å¯¹è¯
        print("\n=== ç¤ºä¾‹1ï¼šåŸºæœ¬å¯¹è¯ ===")
        response = await agent.run("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
        print(f"åŠ©æ‰‹ï¼š{response}")

        # ç¤ºä¾‹2ï¼šæµå¼å“åº”
        print("\n=== ç¤ºä¾‹2ï¼šæµå¼å“åº” ===")
        print("é—®é¢˜ï¼šè¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
        print("åŠ©æ‰‹ï¼š", end="", flush=True)

        async for chunk in agent.run_stream("è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"):
            if chunk["type"] == "content":
                print(chunk["content"], end="", flush=True)
            elif chunk["type"] == "complete":
                print(f"\n[âœ… å®Œæˆï¼Œç”¨æ—¶ {chunk.get('iterations', 1)} è½®è¿­ä»£]")

        # ç¤ºä¾‹3ï¼šä»£ç ç”Ÿæˆ
        print("\n=== ç¤ºä¾‹3ï¼šä»£ç ç”Ÿæˆ ===")
        code_response = await agent.run("å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹")
        print(f"åŠ©æ‰‹ï¼š{code_response}")

        # ç¤ºä¾‹4ï¼šè¿ç»­å¯¹è¯ï¼ˆæµ‹è¯•è®°å¿†åŠŸèƒ½ï¼‰
        print("\n=== ç¤ºä¾‹4ï¼šè¿ç»­å¯¹è¯ ===")
        await agent.run("æˆ‘çš„åå­—æ˜¯å¼ ä¸‰ï¼Œæˆ‘æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ")
        memory_response = await agent.run("ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’ŒèŒä¸šå—ï¼Ÿ")
        print(f"åŠ©æ‰‹ï¼š{memory_response}")

        # æ˜¾ç¤ºæ€§èƒ½ä¿¡æ¯
        print("\n=== æ€§èƒ½ä¿¡æ¯ ===")
        print("âœ… æœ¬åœ°è¿è¡Œï¼šå®Œå…¨ç§å¯†ï¼Œæ— éœ€ç½‘ç»œ")
        print("âœ… æ— APIè´¹ç”¨ï¼šä¸€æ¬¡éƒ¨ç½²ï¼Œæ°¸ä¹…ä½¿ç”¨")
        print("âœ… è‡ªå®šä¹‰æ§åˆ¶ï¼šå¯è°ƒæ•´æ¨¡å‹å‚æ•°")

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        print("å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
        print("1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼šollama list")
        print("2. é‡å¯ Ollama æœåŠ¡ï¼šollama serve")
        print("3. å°è¯•ä¸‹è½½å…¶ä»–æ¨¡å‹ï¼šollama pull llama3.2")

    finally:
        # æ¸…ç†èµ„æº
        await agent.close()
        print("\nğŸ”š ç¤ºä¾‹ç»“æŸ")


async def model_comparison():
    """ä¸åŒæ¨¡å‹æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹ã€‚"""
    models_to_test = ["llama3.2", "llama3.2:7b", "qwen2.5:7b"]
    test_prompt = "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "

    print("\n=== æ¨¡å‹å¯¹æ¯”æµ‹è¯• ===")

    for model in models_to_test:
        try:
            print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model}")

            settings = AgentSettings(
                name=f"test_{model.replace(':', '_')}",
                instruction="è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚",
                llm_settings=LLMSettings(
                    provider="ollama",
                    model=model,
                    base_url="http://localhost:11434",
                    temperature=0.3,
                    max_tokens=100,
                ),
            )

            agent = Agent(config=settings)
            await agent.initialize()

            import time

            start_time = time.time()
            response = await agent.run(test_prompt)
            end_time = time.time()

            print(f"   å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
            print(f"   å›ç­”: {response[:100]}...")

            await agent.close()

        except Exception as e:
            print(f"   âŒ æ¨¡å‹ {model} ä¸å¯ç”¨: {e}")


if __name__ == "__main__":
    # è¿è¡Œä¸»ç¤ºä¾‹
    asyncio.run(main())

    # å¯é€‰ï¼šè¿è¡Œæ¨¡å‹å¯¹æ¯”
    print("\n" + "=" * 50)
    user_input = input("æ˜¯å¦è¿è¡Œæ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼Ÿ(y/n): ")
    if user_input.lower() == "y":
        asyncio.run(model_comparison())
