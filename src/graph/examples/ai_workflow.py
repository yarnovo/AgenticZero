"""
AIå·¥ä½œæµç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨AIèŠ‚ç‚¹åˆ›å»ºæ™ºèƒ½å·¥ä½œæµ
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from src.graph import (
    EnhancedGraph,
    AIRouter,
    AIAnalyzer,
    AIGenerator,
    AIEvaluator,
    IAgent,
    AgentMessage,
    AgentResponse,
    SequenceControlNode,
    ResumableExecutor
)


# æ¨¡æ‹Ÿçš„AI Agentå®ç°
class MockLLMAgent(IAgent):
    """æ¨¡æ‹Ÿçš„å¤§è¯­è¨€æ¨¡å‹Agent"""
    
    def __init__(self, name: str):
        self.name = name
        self.model_info = {"name": name, "type": "mock_llm"}
        
    async def think(self, messages: List[AgentMessage], context: Optional[Dict[str, Any]] = None) -> AgentResponse:
        """æ¨¡æ‹Ÿæ€è€ƒè¿‡ç¨‹"""
        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
        last_message = messages[-1].content if messages else ""
        
        # æ¨¡æ‹Ÿå›å¤
        if "åˆ†æ" in last_message:
            response_content = json.dumps({
                "ä¸»è¦å‘ç°": ["æ•°æ®é‡å¢é•¿20%", "ç”¨æˆ·æ´»è·ƒåº¦æå‡"],
                "å…³é”®æ´å¯Ÿ": ["å­£èŠ‚æ€§å› ç´ æ˜æ˜¾", "éœ€è¦å¢åŠ æœåŠ¡å™¨å®¹é‡"],
                "å»ºè®®è¡ŒåŠ¨": ["ä¼˜åŒ–ç¼“å­˜ç­–ç•¥", "å‡†å¤‡æ‰©å®¹æ–¹æ¡ˆ"]
            }, ensure_ascii=False)
        elif "ç”Ÿæˆ" in last_message:
            response_content = "åŸºäºåˆ†æç»“æœï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹æªæ–½ï¼š\n1. ç«‹å³ä¼˜åŒ–ç¼“å­˜\n2. åˆ¶å®šæ‰©å®¹è®¡åˆ’\n3. ç›‘æ§ç³»ç»Ÿè´Ÿè½½"
        else:
            response_content = f"å·²æ”¶åˆ°è¯·æ±‚ï¼š{last_message[:50]}..."
            
        return AgentResponse(
            content=response_content,
            confidence=0.85,
            reasoning="åŸºäºå†å²æ•°æ®å’Œæ¨¡å¼åˆ†æ"
        )
        
    async def plan(self, goal: str, constraints: Optional[List[str]] = None, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """åˆ¶å®šè®¡åˆ’"""
        return [
            {"step": 1, "action": "æ”¶é›†æ•°æ®", "duration": "1å¤©"},
            {"step": 2, "action": "åˆ†æè¶‹åŠ¿", "duration": "2å°æ—¶"},
            {"step": 3, "action": "ç”ŸæˆæŠ¥å‘Š", "duration": "1å°æ—¶"}
        ]
        
    async def decide(self, options: List[str], criteria: Optional[Dict[str, float]] = None, context: Optional[Dict[str, Any]] = None) -> str:
        """åšå‡ºå†³ç­–"""
        # ç®€å•çš„å†³ç­–é€»è¾‘
        if "high_performance" in options:
            return "high_performance"
        elif "balanced" in options:
            return "balanced"
        return options[0] if options else "default"
        
    async def evaluate(self, subject: Any, criteria: Optional[List[str]] = None, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """è¯„ä¼°ç»“æœ"""
        scores = {}
        for criterion in (criteria or ["è´¨é‡"]):
            # æ¨¡æ‹Ÿè¯„åˆ†
            scores[criterion] = 0.75 + (len(str(subject)) % 10) * 0.025
            
        return {
            "scores": scores,
            "average": sum(scores.values()) / len(scores),
            "recommendation": "ä¼˜ç§€" if sum(scores.values()) / len(scores) > 0.8 else "è‰¯å¥½"
        }


async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå¢å¼ºå›¾
    graph = EnhancedGraph("ai_workflow")
    
    # åˆ›å»ºå…±äº«çš„Agent
    agent = MockLLMAgent("SmartAssistant")
    
    # åˆ›å»ºèŠ‚ç‚¹
    start = SequenceControlNode("start", "å¼€å§‹", lambda x: {"raw_data": x})
    
    # AIåˆ†æèŠ‚ç‚¹
    analyzer = AIAnalyzer(
        "analyzer",
        "æ•°æ®åˆ†æ",
        analysis_type="trend_analysis",
        agent=agent
    )
    
    # AIè·¯ç”±èŠ‚ç‚¹ï¼šæ ¹æ®åˆ†æç»“æœé€‰æ‹©å¤„ç†ç­–ç•¥
    router = AIRouter(
        "router",
        "æ™ºèƒ½è·¯ç”±",
        routes=["optimize", "expand", "monitor"],
        agent=agent
    )
    
    # ä¸åŒç­–ç•¥çš„å¤„ç†èŠ‚ç‚¹
    optimize = AIGenerator(
        "optimize",
        "ä¼˜åŒ–æ–¹æ¡ˆç”Ÿæˆ",
        generation_type="plan",
        agent=agent
    )
    
    expand = AIGenerator(
        "expand",
        "æ‰©å®¹æ–¹æ¡ˆç”Ÿæˆ",
        generation_type="plan",
        agent=agent
    )
    
    monitor = SequenceControlNode(
        "monitor",
        "ç›‘æ§è®¾ç½®",
        lambda x: {"action": "è®¾ç½®ç›‘æ§å‘Šè­¦", "data": x}
    )
    
    # AIè¯„ä¼°èŠ‚ç‚¹
    evaluator = AIEvaluator(
        "evaluator",
        "æ–¹æ¡ˆè¯„ä¼°",
        criteria=["å¯è¡Œæ€§", "æˆæœ¬æ•ˆç›Š", "é£é™©ç¨‹åº¦"],
        agent=agent
    )
    
    # ç»“æŸèŠ‚ç‚¹
    end = SequenceControlNode(
        "end",
        "å®Œæˆ",
        lambda x: {"final_result": x, "status": "completed"}
    )
    
    # æ„å»ºå›¾
    graph.add_node(start)
    graph.add_node(analyzer)
    graph.add_node(router)
    graph.add_node(optimize)
    graph.add_node(expand)
    graph.add_node(monitor)
    graph.add_node(evaluator)
    graph.add_node(end)
    
    # è¿æ¥èŠ‚ç‚¹
    graph.add_edge("start", "analyzer")
    graph.add_edge("analyzer", "router")
    graph.add_edge("router", "optimize", "optimize")
    graph.add_edge("router", "expand", "expand")
    graph.add_edge("router", "monitor", "monitor")
    graph.add_edge("optimize", "evaluator")
    graph.add_edge("expand", "evaluator")
    graph.add_edge("monitor", "end")
    graph.add_edge("evaluator", "end")
    
    # è®¾ç½®èµ·å§‹å’Œç»“æŸèŠ‚ç‚¹
    graph.set_start("start")
    graph.add_end("end")
    
    # éªŒè¯å›¾
    graph.validate()
    
    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = ResumableExecutor(graph)
    
    # è®¾ç½®é’©å­
    async def on_ai_decision(node):
        if hasattr(node, 'agent'):
            print(f"ğŸ¤– AIèŠ‚ç‚¹ {node.node_id} åšå‡ºå†³ç­–")
            
    executor.register_hook("node_complete", on_ai_decision)
    
    # æµ‹è¯•æ•°æ®
    test_data = {
        "metrics": {
            "cpu_usage": 85,
            "memory_usage": 70,
            "request_count": 10000,
            "response_time": 250
        },
        "timestamp": "2024-01-01T10:00:00"
    }
    
    print("=== æ‰§è¡ŒAIå·¥ä½œæµ ===")
    context = await executor.execute_with_checkpoints(initial_input=test_data)
    
    print(f"\næ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {context.duration:.2f}ç§’")
    print(f"è®¿é—®çš„èŠ‚ç‚¹: {list(context.visited_nodes)}")
    
    # æ˜¾ç¤ºAIèŠ‚ç‚¹çš„å¯¹è¯å†å²
    print("\n=== AIå¯¹è¯å†å² ===")
    for node_id, node in graph.nodes.items():
        if hasattr(node, 'conversation_history') and node.conversation_history:
            print(f"\n{node_id} çš„å¯¹è¯:")
            for msg in node.conversation_history:
                role = msg.role
                content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                print(f"  {role}: {content}")
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹å¹¶æ¼”ç¤ºæ¢å¤
    print("\n=== æµ‹è¯•ä¸­æ–­å’Œæ¢å¤ ===")
    # åˆ›å»ºä¸€ä¸ªä¼šä¸­æ–­çš„æ‰§è¡Œ
    graph2 = EnhancedGraph("interruptible_workflow")
    
    # æ·»åŠ ä¸€ä¸ªä¼š"å¤±è´¥"çš„èŠ‚ç‚¹
    class InterruptibleTask(TaskNode):
        def __init__(self, node_id: str, name: str, fail_on_run: int = 1):
            super().__init__(node_id, name)
            self.run_count = 0
            self.fail_on_run = fail_on_run
            
        async def _execute_task(self, input_data):
            self.run_count += 1
            if self.run_count == self.fail_on_run:
                raise Exception("æ¨¡æ‹Ÿçš„ä¸­æ–­")
            return {"processed": input_data, "run": self.run_count}
    
    # æ„å»ºç®€å•çš„å¯ä¸­æ–­æµç¨‹
    task1 = SequenceControlNode("task1", "ä»»åŠ¡1", lambda x: {"step1": x})
    task2 = InterruptibleTask("task2", "å¯ä¸­æ–­ä»»åŠ¡", fail_on_run=1)
    task3 = SequenceControlNode("task3", "ä»»åŠ¡3", lambda x: {"step3": x})
    
    graph2.add_node(task1)
    graph2.add_node(task2)
    graph2.add_node(task3)
    
    graph2.add_edge("task1", "task2")
    graph2.add_edge("task2", "task3")
    
    graph2.set_start("task1")
    graph2.add_end("task3")
    
    executor2 = ResumableExecutor(graph2)
    
    # ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼ˆä¼šå¤±è´¥ï¼‰
    print("\nç¬¬ä¸€æ¬¡æ‰§è¡Œï¼ˆä¼šä¸­æ–­ï¼‰...")
    try:
        await executor2.execute_with_checkpoints(initial_input="test_data")
    except Exception as e:
        print(f"æ‰§è¡Œä¸­æ–­: {e}")
        
    # ä»å¿«ç…§æ¢å¤
    if executor2.snapshots:
        print("\nä»å¿«ç…§æ¢å¤æ‰§è¡Œ...")
        last_snapshot = executor2.snapshots[-1]
        
        # ä¿®å¤"é—®é¢˜"
        task2.fail_on_run = 2
        
        # æ¢å¤æ‰§è¡Œ
        context = await executor2.resume_from_snapshot(last_snapshot)
        print(f"æ¢å¤æ‰§è¡ŒæˆåŠŸï¼æœ€ç»ˆç»“æœ: {context.graph_output}")


if __name__ == "__main__":
    asyncio.run(main())